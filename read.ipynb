{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_urls_and_summaries(CONSTANTS):\n",
    "    \"\"\"Collects all urls and summaries from the website\"\"\"\n",
    "    if os.path.isfile(CONSTANTS['SAVE_PROGRESS']):\n",
    "        # If save file exists, read progress from file\n",
    "        with open(CONSTANTS['SAVE_PROGRESS']) as f:\n",
    "            saved_data = json.load(f)\n",
    "            if saved_data['page'] >= CONSTANTS['end_page'] - 1: # If we have already finished, return False\n",
    "                return True\n",
    "            start_page = saved_data['page'] + 1\n",
    "            urls = saved_data['urls']\n",
    "            category = saved_data['category']\n",
    "            headers_list = saved_data['headers']\n",
    "            author_list = saved_data['authors']\n",
    "    else:\n",
    "        # Otherwise, start from the beginning\n",
    "        start_page = 1\n",
    "        urls = []\n",
    "        category = []\n",
    "        headers_list = []\n",
    "        author_list = []\n",
    "\n",
    "    for page in tqdm(range(start_page, CONSTANTS['end_page'])):\n",
    "        page_url = f'https://scienceblog.com/page/{page}/'\n",
    "        retry = True\n",
    "        while retry:\n",
    "            try:\n",
    "                resp = CONSTANTS['session'].get(page_url, headers=CONSTANTS['headers'])\n",
    "                retry = False\n",
    "            except requests.exceptions.RequestException:\n",
    "                # Wait for 5 seconds and try again\n",
    "                time.sleep(5)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        if not soup.find_all('div', class_='inside-article'):\n",
    "            break\n",
    "        blocks = soup.find_all('div', class_='inside-article')\n",
    "        for block in blocks:\n",
    "            category.append(block.find('footer', class_=\"entry-meta\").a.get_text(strip=True))\n",
    "            headers_list.append(block.find('h2', class_=\"entry-title\").get_text(strip=True))\n",
    "            author_list.append(block.find('div', class_=\"entry-meta\").get_text(strip=True))\n",
    "            urls.append(block.find('header', class_=\"entry-header\").a.get('href'))\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Save progress every SAVE_EVERY steps\n",
    "        if page % CONSTANTS['SAVE_EVERY'] == 0 or page >= CONSTANTS['end_page'] - CONSTANTS['SAVE_EVERY']:\n",
    "            with open(CONSTANTS['SAVE_PROGRESS'], 'w') as f:\n",
    "                data = {\n",
    "                    'page': page,\n",
    "                    'index': len(urls),\n",
    "                    'urls': urls,\n",
    "                    'category': category,\n",
    "                    'headers': headers_list,\n",
    "                    'authors': author_list,\n",
    "                }\n",
    "                json.dump(data, f)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(CONSTANTS, article_urls, category, headers_list, author_list, start_index, articles):\n",
    "    \"\"\" Collects text from the urls \"\"\"\n",
    "    \n",
    "    for i, url in tqdm(enumerate(article_urls[start_index:]), total=len(article_urls[start_index:])):\n",
    "        index = start_index + i\n",
    "        retry = True\n",
    "        \n",
    "        while retry:\n",
    "            try:\n",
    "                resp = CONSTANTS['session'].get(url, headers=CONSTANTS['headers'])\n",
    "                retry = False\n",
    "            except requests.exceptions.RequestException:\n",
    "                # Wait for 5 seconds and try again\n",
    "                time.sleep(5)\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        \n",
    "        article = {\n",
    "            \"url\": url,\n",
    "            \"category\": category[index],\n",
    "            \"header\": headers_list[index],\n",
    "            \"author\": author_list[index],\n",
    "            \n",
    "            \"article_text\": soup.find('div', class_='entry-content').get_text(strip=True),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            article[\"date\"] = soup.find('time', class_='entry-date published').get_text(strip=True)\n",
    "        except:\n",
    "            article[\"date\"] = None\n",
    "        \n",
    "        articles.append(article)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Save progress every SAVE_EVERY steps\n",
    "        if i % CONSTANTS['SAVE_EVERY'] == 0 or i >= len(article_urls[start_index:]) - CONSTANTS['SAVE_EVERY']:\n",
    "            with open(CONSTANTS['SAVE_FILE'], 'w') as f:\n",
    "                data = {\n",
    "                    'index': index,\n",
    "                    'articles': articles\n",
    "                }\n",
    "                json.dump(data, f, indent=4)\n",
    "    \n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_the_articles(CONSTANTS):\n",
    "    \"\"\" Collects all articles \"\"\"\n",
    "    \n",
    "    collect_urls_and_summaries(CONSTANTS) # Collect urls and summaries\n",
    "    \n",
    "    with open(CONSTANTS['SAVE_PROGRESS']) as f: # Read urls and summaries\n",
    "        saved_progress = json.load(f)\n",
    "        urls = saved_progress['urls']\n",
    "        category = saved_progress['category']\n",
    "        headers_list = saved_progress['headers']\n",
    "        author_list = saved_progress['authors']\n",
    "    \n",
    "    if os.path.isfile(CONSTANTS['SAVE_FILE']): # If save file exists, read progress from file\n",
    "        with open(CONSTANTS['SAVE_FILE']) as f:\n",
    "            saved_data = json.load(f)\n",
    "            start_index = saved_data['index']\n",
    "            if start_index == saved_progress['index']: # If we have already finished, return \n",
    "                return 'Already finished'\n",
    "            articles = saved_data['articles']\n",
    "            \n",
    "    else: # Otherwise, start from the beginning\n",
    "        start_index = 0\n",
    "        articles = []\n",
    "        \n",
    "    return collect_text(CONSTANTS, urls, category, headers_list,\n",
    "                    author_list, start_index, articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "CONSTANTS = {\n",
    "    'SAVE_EVERY': 10,\n",
    "    'SAVE_PROGRESS': 'progress.json',\n",
    "    'SAVE_FILE': 'articles.json',\n",
    "    'headers': {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    "    },\n",
    "    'session': requests.Session(),\n",
    "    'end_page': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/429 [00:18<11:55,  1.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m collect_the_articles(CONSTANTS)\n",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m, in \u001b[0;36mcollect_the_articles\u001b[0;34m(CONSTANTS)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect_the_articles\u001b[39m(CONSTANTS):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Collects all articles \"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     collect_urls_and_summaries(CONSTANTS) \u001b[39m# Collect urls and summaries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(CONSTANTS[\u001b[39m'\u001b[39m\u001b[39mSAVE_PROGRESS\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mas\u001b[39;00m f: \u001b[39m# Read urls and summaries\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         saved_progress \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n",
      "Cell \u001b[0;32mIn[60], line 41\u001b[0m, in \u001b[0;36mcollect_urls_and_summaries\u001b[0;34m(CONSTANTS)\u001b[0m\n\u001b[1;32m     39\u001b[0m     author_list\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mentry-meta\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget_text(strip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     40\u001b[0m     urls\u001b[39m.\u001b[39mappend(block\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mentry-header\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Save progress every SAVE_EVERY steps\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m page \u001b[39m%\u001b[39m CONSTANTS[\u001b[39m'\u001b[39m\u001b[39mSAVE_EVERY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m page \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m CONSTANTS[\u001b[39m'\u001b[39m\u001b[39mend_page\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m CONSTANTS[\u001b[39m'\u001b[39m\u001b[39mSAVE_EVERY\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "collect_the_articles(CONSTANTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "  2%|▏         | 6/276 [00:07<05:58,  1.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 108\u001b[0m\n\u001b[1;32m    103\u001b[0m     article[\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    107\u001b[0m articles\u001b[39m.\u001b[39mappend(article)\n\u001b[0;32m--> 108\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    110\u001b[0m \u001b[39m# Save progress every SAVE_EVERY steps\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m SAVE_EVERY \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(article_urls[start_index:]) \u001b[39m-\u001b[39m SAVE_EVERY:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Define constants\n",
    "# SAVE_EVERY = 10\n",
    "# SAVE_PROGRESS = 'progress.json'\n",
    "# SAVE_FILE = 'articles.json'\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# end_page = 20\n",
    "\n",
    "\n",
    "# # Step 1: Collect article urls and summaries\n",
    "# if os.path.isfile(SAVE_PROGRESS):\n",
    "#     # If save file exists, read progress from file\n",
    "#     with open(SAVE_PROGRESS) as f:\n",
    "#         saved_progress = json.load(f)\n",
    "#         if saved_progress['page'] >= end_page:\n",
    "#             print('Already finished!')\n",
    "#             exit()\n",
    "#         start_page = saved_progress['page']\n",
    "#         start_index = saved_progress['index']\n",
    "#         urls = saved_progress['urls']\n",
    "#         category = saved_progress['category']\n",
    "#         headers_list = saved_progress['headers']\n",
    "#         author_list = saved_progress['authors']\n",
    "# else:\n",
    "#     # Otherwise, start from the beginning\n",
    "#     start_page = 1\n",
    "#     start_index = 0\n",
    "#     urls = []\n",
    "#     category = []\n",
    "#     headers_list = []\n",
    "#     author_list = []\n",
    "\n",
    "# for page in tqdm(range(start_page, end_page)):\n",
    "#     page_url = f'https://scienceblog.com/page/{page}/'\n",
    "#     retry = True\n",
    "#     while retry:\n",
    "#         try:\n",
    "#             resp = session.get(page_url, headers=headers)\n",
    "#             retry = False\n",
    "#         except requests.exceptions.RequestException:\n",
    "#             # Wait for 5 seconds and try again\n",
    "#             time.sleep(5)\n",
    "#     soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "#     if not soup.find_all('div', class_='inside-article'):\n",
    "#         break\n",
    "#     blocks = soup.find_all('div', class_='inside-article')\n",
    "#     for block in blocks:\n",
    "#         category.append(block.find('footer', class_=\"entry-meta\").a.get_text(strip=True))\n",
    "#         headers_list.append(block.find('h2', class_=\"entry-title\").get_text(strip=True))\n",
    "#         author_list.append(block.find('div', class_=\"entry-meta\").get_text(strip=True))\n",
    "#         urls.append(block.find('header', class_=\"entry-header\").a.get('href'))\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     # Save progress every SAVE_EVERY steps\n",
    "#     if page % SAVE_EVERY == 0 or page >= end_page - SAVE_EVERY:\n",
    "#         with open(SAVE_PROGRESS, 'w') as f:\n",
    "#             data = {\n",
    "#                 'page': page,\n",
    "#                 'index': len(urls),\n",
    "#                 'urls': urls,\n",
    "#                 'category': category,\n",
    "#                 'headers': headers_list,\n",
    "#                 'authors': author_list,\n",
    "#             }\n",
    "#             json.dump(data, f)\n",
    "\n",
    "# # Step 2: Loop over article urls and scrape information\n",
    "# if os.path.isfile(SAVE_FILE):\n",
    "#     # If save file exists, read progress from file\n",
    "#     with open(SAVE_FILE) as f:\n",
    "#         saved_progress = json.load(f)\n",
    "#         start_index = saved_progress['index']\n",
    "#         articles = saved_progress['articles']\n",
    "# else:\n",
    "#     start_index = 0\n",
    "#     articles = []\n",
    "    \n",
    "\n",
    "# for i, url in tqdm(enumerate(urls[start_index:]), total=len(urls[start_index:])):\n",
    "#     index = start_index + i\n",
    "#     retry = True\n",
    "#     while retry:\n",
    "#         try:\n",
    "#             resp = session.get(url, headers=headers)\n",
    "#             retry = False\n",
    "#         except requests.exceptions.RequestException:\n",
    "#             # Wait for 5 seconds and try again\n",
    "#             time.sleep(5)\n",
    "#     soup = BeautifulSoup(resp.content, 'lxml')\n",
    "    \n",
    "#     article = {\n",
    "#         \"url\": url,\n",
    "#         \"category\": category[index],\n",
    "#         \"header\": headers_list[index],\n",
    "#         \"author\": author_list[index],\n",
    "        \n",
    "#         \"article_text\": soup.find('div', class_='entry-content').get_text(strip=True),\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         article[\"date\"] = soup.find('time', class_='entry-date published').get_text(strip=True)\n",
    "#     except:\n",
    "#         article[\"date\"] = None\n",
    "    \n",
    "    \n",
    "    \n",
    "#     articles.append(article)\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     # Save progress every SAVE_EVERY steps\n",
    "#     if i % SAVE_EVERY == 0 or i >= len(urls[start_index:]) - SAVE_EVERY:\n",
    "#         with open(SAVE_FILE, 'w') as f:\n",
    "#             data = {\n",
    "#                 'index': index,\n",
    "#                 'articles': articles\n",
    "#             }\n",
    "#             json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('progress.json')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urls'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mpotsawee/t5-large-generation-race-QuestionAnswer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mpotsawee/t5-large-generation-race-QuestionAnswer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m context \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39marticle_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m:\u001b[39m100\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(context)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds3.11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1086\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1086\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds3.11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1074\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1072\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1073\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "\n",
    "context = df['article_text'][1][0:100]\n",
    "print(context)\n",
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = question_answer.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)\n",
    "\n",
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "437d2f9cc7c9758e5462207d00477b27948c8c7181cefeb9cbc63204a2341966"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
